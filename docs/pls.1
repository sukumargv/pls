.TH PLS 1 "February 2026" "pls 0.1.0" "User Commands"
.SH NAME
pls \- Natural language to shell commands using Ollama
.SH SYNOPSIS
.B pls
[\fIOPTIONS\fR] \fIPROMPT\fR [\fISHELL\fR]

.B pls-engine
[\fIOPTIONS\fR] \fIPROMPT\fR [\fISHELL\fR]

.SH DESCRIPTION
.B pls
converts natural language descriptions into shell commands using local Large Language Models via Ollama.
It supports multiple shells (Bash, Zsh, Fish) and runs entirely offline for enhanced privacy.

The command is generated by Ollama and is presented to you for review before execution.
You can edit the suggested command before running it, or cancel with Ctrl+C.

.SH OPTIONS
.TP
.B -h, --help
Display help information and exit.

.TP
.B -v, --version
Display version information and exit.

.TP
.B --fast, -f
Use fast mode: skip context analysis and generate command directly.
Faster but may produce less accurate results for complex commands.

.TP
.B --verbose
Enable verbose output with status messages about what pls is doing.
Shows connection status, execution mode, and response processing updates.

.TP
.B --debug
Enable detailed debug output including full JSON payloads sent to Ollama.
Useful for troubleshooting and understanding what's being sent to the AI model.

.SH ARGUMENTS
.TP
.B PROMPT
Natural language description of the desired shell command.
This is the primary input and is required.

Examples:
.RS
.TP
"find all log files larger than 10MB"
.TP
"compress all jpg files in this directory"
.TP
"list running python processes"
.RE

.TP
.B SHELL
Target shell for command generation: \fBbash\fR, \fBzsh\fR, or \fBfish\fR.
If not specified, pls attempts to detect your current shell.
If detection fails, defaults to bash.

.SH EXAMPLES
.TP
Basic usage:
.RS
pls "find large log files"
.RE

.TP
Fast mode for quick responses:
.RS
pls --fast "list all files sorted by size"
.RE

.TP
Verbose mode to see what's happening:
.RS
pls --verbose "grep pattern in files"
.RE

.TP
Debug mode with full information:
.RS
pls --debug "kill all idle processes"
.RE

.TP
Specify target shell:
.RS
pls "list directory contents" zsh
.RE

.TP
Combination of options:
.RS
pls --fast --verbose "find modified files" fish
.RE

.SH ENVIRONMENT
.TP
.B HOME
Used to locate configuration directory: $HOME/.config/pls/config.json

.TP
.B SHELL
Used to auto-detect the current shell (if shell argument not provided).

.SH FILES
.TP
.B ~/.config/pls/config.json
User configuration file. Created automatically on first run with sensible defaults.

.TP
.B ~/.config/pls/
Configuration directory for pls.

.SH CONFIGURATION
On first run, pls creates a configuration file at \fB~/.config/pls/config.json\fR with reasonable defaults.

.SS Configuration Options
.TP
.B model
The Ollama model to use for generation. Default: \fBgemma3:4b\fR
Examples: codellama:13b, codegemma:7b, deepseek-coder:6.7b

.TP
.B ollama_url
URL where Ollama is running. Default: \fBhttp://localhost:11434\fR
Change if using a remote Ollama instance or custom port.

.TP
.B temperature
LLM temperature (0.0-1.0). Lower = more deterministic. Default: \fB0.1\fR

.TP
.B system_prompt
Base system prompt given to the model. Customize to change behavior globally.

.TP
.B shell_specific_prompts
Additional prompts for specific shells (fish, bash, zsh).

.TP
.B max_context_files
Maximum files to scan in context-aware mode. Default: \fB20\fR

.TP
.B max_help_text_length
Maximum characters to capture from command help text. Default: \fB4096\fR

For detailed configuration documentation, see \fBdocs/configuration.md\fR.

.SH MODES
.TP
.B Context-Aware Mode (Default)
Multi-step process:
.RS
.IP 1. 4
Scans current directory for file context
.IP 2. 4
Identifies relevant command from user request
.IP 3. 4
Retrieves help text for that command
.IP 4. 4
Generates command with full context
.RE

More accurate but slower (~8-14 seconds).

.TP
.B Fast Mode (--fast)
Single LLM call without context analysis.
Faster (~3-5 seconds) but may be less accurate for complex commands.
Automatically used if context mode encounters errors.

.SH PREREQUISITES
.TP
.B Ollama
Must be running locally. Download from https://ollama.ai

.TP
.B Language Model
At least one Ollama model installed. Install with: \fBollama pull model-name\fR

Recommended models:
.RS
.TP
gemma3:4b (fast, recommended default)
.TP
codellama:13b (best balance)
.TP
deepseek-coder:6.7b (excellent for complex tasks)
.RE

.TP
.B Dependencies
- \fBjq\fR - JSON processor
- \fBcurl\fR - HTTP client
- \fBbash\fR 4.0 or higher

.SH EXIT STATUS
.TP
.B 0
Command successfully generated and output to stdout.

.TP
.B 1
Error occurred. Check stderr for error message.
Common reasons: missing dependencies, cannot connect to Ollama, no model available.

.SH SECURITY CONSIDERATIONS
.B pls
runs entirely offline using local Ollama. No data is sent to external services.

When using context-aware mode, pls:
1. Validates identified command names for safety
2. Prompts before running `--help` on identified commands
3. Sanitizes all input before sending to Ollama

.SH TROUBLESHOOTING
.TP
.B "Cannot connect to Ollama"
Make sure Ollama is running: \fBollama serve\fR

.TP
.B "Model not found"
Pull the required model: \fBollama pull gemma3:4b\fR

.TP
.B "jq not found"
Install jq using your package manager:
.RS
Arch/Manjaro: pacman -S jq
Ubuntu/Debian: apt install jq
macOS: brew install jq
.RE

.TP
.B Slow responses
Try a smaller model like gemma3:4b or use --fast mode.

.TP
.B Inaccurate commands
Try using a larger model (codellama:13b, deepseek-coder:6.7b) or use context-aware mode (default).

.TP
.B "sh: command not found" after running generated command
The generated command may require flags that weren't used. Run with \fB--verbose\fR to see what was generated.

For more help, use:
.RS
pls --help
pls --debug "your command"
.RE

.SH SIMILAR PROJECTS
.TP
.B uwu
https://github.com/context-labs/uwu - Uses OpenAI GPT models

.TP
.B shell_gpt
https://github.com/TheR1D/shell_gpt - Another AI shell assistant

.TP
.B butterfish
https://butterfi.sh - CLI tools for LLMs

.SH AUTHOR
pls is developed by the community. See LICENSE for details.

.SH SEE ALSO
.B ollama(1)
- The Ollama AI model server.

.B bash(1)
- Default shell.

.B zsh(1)
- Advanced shell.

.B fish(1)
- Friendly interactive shell.

More information and documentation at:
https://github.com/GaelicThunder/pls

Configuration guide:
https://github.com/GaelicThunder/pls/blob/master/docs/configuration.md

API documentation:
https://github.com/GaelicThunder/pls/blob/master/docs/API.md

.SH BUGS
Report bugs at: https://github.com/GaelicThunder/pls/issues

.SH LICENSE
MIT License - See LICENSE file for details
