#!/usr/bin/env bash
# pls-engine: Streams to stderr, outputs single clean command to stdout
set -uo pipefail

# --- Globals ---
CONFIG_DIR="$HOME/.config/pls"
CONFIG_FILE="$CONFIG_DIR/config.json"
RED="\033[0;31m"
GREEN="\033[0;32m"
YELLOW="\033[0;33m"
BLUE="\033[0;34m"
NC="\033[0m"
FRAMES=("⠋" "⠙" "⠹" "⠸" "⠼" "⠴" "⠦" "⠧" "⠇" "⠏")

# --- Logger and Utilities ---
_DEBUG_MODE=0
debug() { [ "${_DEBUG_MODE}" -eq 1 ] && echo -e "${YELLOW}[DEBUG]${NC} $*" >&2; }
error() { echo -e "${RED}[ERROR]${NC} $*" >&2; }
info() { echo -e "${BLUE}[INFO]${NC} $*" >&2; }

show_thinking() {
  local pid=$1
  local i=0
  while kill -0 "$pid" 2>/dev/null; do
    printf "\r${BLUE}${FRAMES[i]} Generating...${NC}" >&2
    ((i = (i + 1) % ${#FRAMES[@]}))
    sleep 0.1
  done
  printf "\r\033[K" >&2
}

# --- Configuration ---
create_default_config() {
  mkdir -p "$CONFIG_DIR"
  cat > "$CONFIG_FILE" << EOL
{
  "model": "gemma3:4b",
  "ollama_url": "http://localhost:11434",
  "temperature": 0.1,
  "system_prompt": "You are a shell command generator. Output exactly one single-line command for the requested task. No explanations, no markdown, no code fences, no language tags. Only the raw command.",
  "shell_specific_prompts": {
    "fish": "Generate Fish shell commands using Fish-specific syntax (e.g., 'set' for variables, Fish conditionals).",
    "bash": "Generate Bash shell commands using POSIX syntax.",
    "zsh": "Generate Zsh shell commands using Zsh-specific features if beneficial."
  }
}
EOL
}

# --- Ollama Communication ---
stream_response() {
  local model="$1" url="$2" prompt="$3" temp="$4"
  local json_payload
  json_payload=$(jq -n \
    --arg model "$model" \
    --arg prompt "$prompt" \
    --argjson stream true \
    --argjson temp "$temp" \
    '{model: $model, prompt: $prompt, stream: $stream, options: {temperature: $temp}}')
  
  debug "JSON Payload to Ollama:"
  [ "${_DEBUG_MODE}" -eq 1 ] && echo "$json_payload" | jq . >&2

  local response_file
  response_file=$(mktemp)
  
  if ! curl -s --head "$url" | head -n 1 | grep "200 OK" > /dev/null; then
      error "Cannot connect to Ollama at $url. Is 'ollama serve' running?"
      rm -f "$response_file"
      return 1
  fi
  
  curl -s "$url/api/generate" -d "$json_payload" > "$response_file" &
  local curl_pid=$!

  show_thinking "$curl_pid"
  wait "$curl_pid"
  
  if first_line=$(head -n 1 "$response_file"); ollama_error=$(echo "$first_line" | jq -r '.error // empty'); [ -n "$ollama_error" ]; then
    error "Ollama API error: $ollama_error"
    rm -f "$response_file"
    return 1
  fi

  local command=""
  while IFS= read -r line; do
    local part
    part=$(echo "$line" | jq -r '.response // empty')
    if [ -n "$part" ]; then
      printf "%s" "$part" >&2
      command+="$part"
    fi
    local done_flag
    done_flag=$(echo "$line" | jq -r '.done // false')
    if [ "$done_flag" = "true" ]; then break; fi
  done < "$response_file"

  printf "\n" >&2
  debug "Raw command from model: '$command'"

  if [ -z "$command" ]; then
    error "No command generated by the model."
    rm -f "$response_file"
    return 1
  fi
  rm -f "$response_file"
  
  command=$(echo "$command" | sed -e 's/`//g' -e 's/^shell//' -e 's/^bash//' -e 's/^fish//' -e 's/^zsh//' -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')
  
  echo "$command"
}

# --- Command Generation Logics ---

generate_command_fast() {
  local user_prompt="$1"
  local current_shell="$2"
  
  debug "Using fast mode (single LLM call)."

  local model=$(jq -r '.model' "$CONFIG_FILE")
  local ollama_url=$(jq -r '.ollama_url' "$CONFIG_FILE")
  local temperature=$(jq -r '.temperature' "$CONFIG_FILE")
  local system_prompt=$(jq -r '.system_prompt' "$CONFIG_FILE")
  local shell_specific=$(jq -r ".shell_specific_prompts.\"$current_shell\" // empty" "$CONFIG_FILE")

  local final_prompt="$system_prompt $shell_specific User request: $user_prompt"

  debug "Model: $model"
  debug "Ollama URL: $ollama_url"
  debug "Temperature: $temperature"

  info "Generating command for $current_shell..."
  stream_response "$model" "$ollama_url" "$final_prompt" "$temperature"
}

generate_command_with_context() {
  local user_prompt="$1"
  local current_shell="$2"
  
  debug "Using context-aware mode (multi-step)."

  local model=$(jq -r '.model' "$CONFIG_FILE")
  local ollama_url=$(jq -r '.ollama_url' "$CONFIG_FILE")
  local temperature=$(jq -r '.temperature' "$CONFIG_FILE")
  local system_prompt=$(jq -r '.system_prompt' "$CONFIG_FILE")
  local shell_specific=$(jq -r ".shell_specific_prompts.\"$current_shell\" // empty" "$CONFIG_FILE")

  # Step 1: Identify relevant commands
  info "Identifying relevant commands..."
  local relevance_prompt="Based on the following user request, what is the most relevant command-line tool to solve the problem? List the single most relevant command name, and nothing else. If no command is relevant, output 'unknown'. Do not include any explanations or formatting. User request: $user_prompt"
  
  local relevant_cmd
  relevant_cmd=$(stream_response "$model" "$ollama_url" "$relevance_prompt" "$temperature")
  
  if [ -z "$relevant_cmd" ] || [[ "$relevant_cmd" == "unknown" ]]; then
    info "Could not identify a relevant command. Falling back to fast mode."
    generate_command_fast "$user_prompt" "$current_shell"
    return
  fi
  
  debug "Identified relevant command: $relevant_cmd"

  # Step 2: Get help text for the command
  info "Getting help text for '$relevant_cmd'..."
  local help_text
  if ! help_text=$($relevant_cmd --help 2>&1); then
    info "Could not get help text for '$relevant_cmd'. Falling back to fast mode."
    generate_command_fast "$user_prompt" "$current_shell"
    return
  fi

  debug "Help text captured."

  # Step 3: Construct final prompt and generate command
  info "Generating command with context..."
  
  # Truncate help text to avoid exceeding context window limits
  local max_len=4096
  if [ "${#help_text}" -gt "$max_len" ]; then
    help_text="${help_text:0:$max_len}"
    debug "Truncated help text to $max_len characters."
  fi

  local final_prompt="$system_prompt $shell_specific \n\nHere is the help text for the relevant command '$relevant_cmd':\n\n$help_text\n\nUser request: $user_prompt"
  
  stream_response "$model" "$ollama_url" "$final_prompt" "$temperature"
}


# --- Main Execution ---
main() {
  local use_fast_mode=0
  local user_prompt=""
  local current_shell="bash"
  
  # Argument parsing
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --debug) _DEBUG_MODE=1; shift ;;
      -v|--version)
        # Simplified version handling for now
        echo "pls-engine v0.1.0 (dev)" # Placeholder
        exit 0
        ;;
      -f|--fast) use_fast_mode=1; shift ;;
      *)
        if [[ -z "$user_prompt" ]]; then
          user_prompt="$1"
        else
          current_shell="$1"
        fi
        shift
        ;;
    esac
  done

  debug "User prompt received: '$user_prompt'"
  debug "Shell context: $current_shell"
  debug "Fast mode: $use_fast_mode"

  # Pre-flight checks
  [ -z "$user_prompt" ] && { error "Usage: pls-engine \"<prompt>\" [shell]"; exit 1; }
  command -v jq >/dev/null || { error "jq required"; exit 1; }
  command -v curl >/dev/null || { error "curl required"; exit 1; }
  [ ! -f "$CONFIG_FILE" ] && create_default_config

  local suggested_cmd=""
  if [ "$use_fast_mode" -eq 1 ]; then
    suggested_cmd=$(generate_command_fast "$user_prompt" "$current_shell")
  else
    suggested_cmd=$(generate_command_with_context "$user_prompt" "$current_shell")
  fi
  
  local exit_code=$?
  if [ $exit_code -ne 0 ]; then
    error "Failed to generate command."
    exit $exit_code
  fi

  if [ -z "$suggested_cmd" ]; then
    error "No command was generated."
    exit 1
  fi
  
  echo "$suggested_cmd"
}

main "$@"
